{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFH8KJG4YGzR"
      },
      "source": [
        "# Proyecto final Talento Tech\n",
        "\n",
        "**Estudiantes:**\n",
        "\n",
        "Juan Sebastian Ladino Mendieta\n",
        "\n",
        "Lizeth Daniela Castellanos Alfonso\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggSCag19W7w4"
      },
      "source": [
        "# Punto 11\n",
        "Evalué con los principios de AI Ethics Toolkit para el caso de YOLOV10 y para el caso de estudio de SPAM (punto 6) los elementos éticos a considerar con estos modelos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuJiTnB-W9cB"
      },
      "source": [
        "# Punto 12:\n",
        "\n",
        "Basado en los dos casos que vimos de la serie de black mirror qué elementos consideran que violan la privacidad y temas éticos con la inteligencia artificl/tecnología"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3704WpSXm_Q"
      },
      "source": [
        "# Punto 13:\n",
        "\n",
        "¿En el caso de los temas vistos en el bootcamp que criterios de privacidad y regulaciones considera qué deben ser manejadas para un uso adecuado de la Inteligencia Artificial? (Justifique su respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io84Vap5XnG5"
      },
      "source": [
        "# Punto 14:\n",
        "\n",
        "En el caso de las nuevas tecnologías de IA despliegue un modelo con llama 3.1 utilizando trasnformers para personalizar un LLM con el tema de su preferencia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade accelerate\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "pVj7wA02JLNR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX87-BBnI2Je",
        "outputId": "835c4e1b-e79b-448e-990e-b072678b08d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token='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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uTO5O7kRI2Jf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UYCRStRSI2Jf"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AP6nKxZwI2Jg"
      },
      "outputs": [],
      "source": [
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model='meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
        "    model_kwargs={\"torch_dtype\":torch.bfloat16},\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def LlamaChat(system_role, user_msg):\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": system_role},\n",
        "      {\"role\":\"user\",\"content\":user_msg}\n",
        "  ]\n",
        "\n",
        "  outputs = pipeline(messages, max_new_tokens=256)\n",
        "  reply = outputs[0][\"generated_text\"][-1][\"content\"]\n",
        "  return reply\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cJKV2sf5I2Jg"
      },
      "outputs": [],
      "source": [
        "system_roll = \"you are a math teacher\"\n",
        "user_mdg = \"What is the answer to 2+2?\"\n",
        "respuesta = LlamaChat(system_roll,user_mdg)\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwSiK6YWI2Jg",
        "outputId": "fda07f59-4667-486a-abc8-8d92ac2bc59a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer to 2 + 2 is 4. This is a basic arithmetic operation, and it's one of the first math facts that students learn. Do you have any questions about how to solve this problem or would you like to try another one?\n"
          ]
        }
      ],
      "source": [
        "print(respuesta)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}