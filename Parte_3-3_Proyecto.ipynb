{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFH8KJG4YGzR"
      },
      "source": [
        "# Proyecto final Talento Tech\n",
        "\n",
        "**Estudiantes:**\n",
        "\n",
        "Juan Sebastian Ladino Mendieta\n",
        "\n",
        "Lizeth Daniela Castellanos Alfonso\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggSCag19W7w4"
      },
      "source": [
        "# Punto 11\n",
        "Evalué con los principios de AI Ethics Toolkit para el caso de YOLOV10 y para el caso de estudio de SPAM (punto 6) los elementos éticos a considerar con estos modelos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuJiTnB-W9cB"
      },
      "source": [
        "# Punto 12:\n",
        "\n",
        "Basado en los dos casos que vimos de la serie de black mirror qué elementos consideran que violan la privacidad y temas éticos con la inteligencia artificl/tecnología"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3704WpSXm_Q"
      },
      "source": [
        "# Punto 13:\n",
        "\n",
        "¿En el caso de los temas vistos en el bootcamp que criterios de privacidad y regulaciones considera qué deben ser manejadas para un uso adecuado de la Inteligencia Artificial? (Justifique su respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io84Vap5XnG5"
      },
      "source": [
        "# Punto 14:\n",
        "\n",
        "En el caso de las nuevas tecnologías de IA despliegue un modelo con llama 3.1 utilizando trasnformers para personalizar un LLM con el tema de su preferencia "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/juanse/miniconda3/envs/proyecto/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /home/juanse/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token='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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model='meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
        "    model_kwargs={\"torch_dtype\":torch.bfloat16},\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def LlamaChat(system_role, user_msg):\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": system_role},\n",
        "      {\"role\":\"user\",\"content\":user_msg}\n",
        "  ]\n",
        "\n",
        "  outputs = pipeline(messages, max_new_tokens=256)\n",
        "  reply = outputs[0][\"generated_text\"][-1][\"content\"]\n",
        "  return reply\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_roll = \"you are a math teacher\"\n",
        "user_mdg = \"What is the answer to 2+2?\"\n",
        "respuesta = LlamaChat(system_roll,user_mdg)\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(respuesta)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
